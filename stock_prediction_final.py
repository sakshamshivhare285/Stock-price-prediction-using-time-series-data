# -*- coding: utf-8 -*-
"""stock prediction final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSx4wBR0KXpQ14CqfRBSIZH53CfRATFh
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import datetime
from datetime import date
from matplotlib.pylab import rcParams
rcParams['figure.figsize']=15,6
import nltk
import re
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.preprocessing import MinMaxScaler
import nltk
nltk.download('vader_lexicon')
!pip install pyramid.arima
!pip install pmdarima
from pmdarima import auto_arima

stock_data=pd.read_csv('/content/drive/MyDrive/Uniqlo(FastRetailing) 2012-2016 Training - stocks2012-2016.csv')

stock_data['Date']=pd.to_datetime(stock_data['Date'])

stock_data.info()

stock_data.describe()

stock_data.drop(['Stock Trading','Volume','Open','High','Low'], axis=1, inplace=True)

stock_data.isnull().sum()

##plotting the data
plt.figure(figsize=(20,15))
stock_data['Close'].plot()
plt.ylabel('stock close price')

##plotting the moving averge
ma=stock_data['Close'].rolling(window=12).mean()
std=stock_data['Close'].rolling(window=12).std()
plt.figure(figsize=(20,10))
stock_data['Close'].plot(color='red', label='Closing data')
ma.plot(color='green', label='moving average')
std.plot(color='blue', label='standar deviation')
plt.legend()

returns=stock_data['Close']/stock_data['Close'].shift(1)-1
returns.plot(color='red', label='returns')
plt.legend()

stock_data.dropna(inplace=True)

## testing the stationarity
from statsmodels.tsa.stattools import adfuller
def test_stationary(timeseries):
  rolmean=timeseries.rolling(20).mean()
  rolstd=timeseries.rolling(20).std()
  orig=plt.plot(timeseries, color='blue', label='origin')
  mean=plt.plot(rolmean,color='red', label='rolmean')
  std=plt.plot(rolstd, color='green', label='rolstd')
  plt.legend(loc='best')
  plt.title('roll mean and roll std')
  plt.show(block=False)
  ## test
  results=adfuller(timeseries, autolag='AIC')
  labels=['Test_stat','p_value','lag used','No. of observation']
  for value,label in zip(results,labels):
     print(label+' : '+str(value) )
     if results[1] <= 0.05:
       print("Strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data is stationary")
     else:
      print("Weak evidence against null hypothesis, time series is non-stationary")

test_stationary(stock_data['Close'])

stock_data_log=np.log(stock_data['Close'])

stock_data_log_diff=stock_data_log-stock_data_log.shift()

stock_data_log_diff.dropna(inplace=True)

test_stationary(stock_data_log_diff)

from sklearn.model_selection import train_test_split
train,test=train_test_split(stock_data_log_diff, test_size=0.3, random_state=44)

from pmdarima import auto_arima

model=auto_arima(train,trace=True, error_action='ignore',suppress_warnings=True)
model.fit(train)

predictions=model.predict(n_periods=len(test))
predictions=pd.DataFrame(predictions, index=test, columns=['predictions'])

test

plt.plot(train, label='Train')
plt.plot(test, label='Test')
plt.plot(predictions, label='Prediction')
plt.title('BSESN Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Actual Stock Price')

from sklearn.metrics import mean_squared_error
ms=(mean_squared_error(test,predictions))
rms=np.sqrt(ms)
print(rms)

##loading the  textual data

textual_data=pd.read_csv('/content/drive/MyDrive/india-news-headlines.csv')

textual_data.head()

textual_data=textual_data.rename(columns={'publish_date':'Date', 'headline_text':'news'})
textual_data.drop(['headline_category'], inplace=True, axis=1)

textual_data['Date']=pd.to_datetime(textual_data['Date'], format='%Y%m%d')

textual_data.info()

textual_data['news']=textual_data.groupby(['Date']).transform(lambda x: ' '.join(x))

textual_data=textual_data.drop_duplicates()
textual_data.reset_index(inplace=True, drop=True)

textual_data

## functions for subjectivity and  polarity
def getsubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

def getpolarity(text):
  return TextBlob(text).sentiment.polarity

textual_data['subjectivity']=textual_data['news'].apply(getsubjectivity)

textual_data['polarity']=textual_data['news'].apply(getpolarity)

sia=SentimentIntensityAnalyzer()

textual_data['Negative'] = [sia.polarity_scores(v)['neg'] for v in textual_data['news']]

textual_data['Neutral'] = [sia.polarity_scores(v)['neu'] for v in textual_data['news']]

textual_data['Positive'] = [sia.polarity_scores(v)['pos'] for v in textual_data['news']]

textual_data['Compound'] = [sia.polarity_scores(v)['compound'] for v in textual_data['news']]

textual_data

data_merge=pd.merge(stock_data,textual_data, how='inner',on='Date')

train_merge=data_merge[['Close','subjectivity', 'polarity', 'Compound', 'Negative', 'Neutral', 'Positive']]

train_merge.head()

scaler=MinMaxScaler()
df=pd.DataFrame(scaler.fit_transform(train_merge))
df.columns=train_merge.columns
df.index=train_merge.index
df.head()

X=df.drop(['Close'], axis=1)
Y=df['Close']

xtrain,xtest,ytrain,ytest=train_test_split(X,Y, test_size=0.3, random_state=44)

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor()

rf.fit(xtrain,ytrain)

prediction=rf.predict(xtest)

print(prediction[:10])
print(ytest[:10])
print('Mean Squared error: ',mean_squared_error(prediction,ytest))

## using adaboost
from sklearn.ensemble import AdaBoostRegressor
adb=AdaBoostRegressor()
adb.fit(xtrain,ytrain)

Predictions_Ada=adb.predict(xtest)

print(mean_squared_error(Predictions_Ada, ytest))

import xgboost
xgb=xgboost.XGBRegressor()

xgb.fit(xtrain,ytrain)

prediction_xgb=xgb.predict(xtest)

print(mean_squared_error(prediction_xgb, ytest))



"""From the above we can conclude that XGboost hold the best results"""

